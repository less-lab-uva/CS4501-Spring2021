---
title: Lab E
subtitle: Robotics and Ethics
layout: page
---

# Robotics and Ethics

[Ethics](https://plato.stanford.edu/entries/ethics-ai/) -- the moral principles that govern a person's conduct or engagement in an activity -- 
and its role in tech is becoming a more widely and contentiously debated topic.
[Robotics often sits at the heart of that debate](https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/study-shows-that-humans-attribute-morals-and-emotions-to-robots) because it’s a highly experimental field, 
full of emerging technology from “internet of things“ to “AI”, 
and lends itself to open-ended development with tons of room for growth accompanied by unclear social-economical implications for the future.


For example, consider the following question: should we keep full autonomy out of the reach of robots? 
On one hand, if the autonomous capabilities in question involve high levels of risk, such as using deadly force or making impactful decisions about quality of life, full autonomy may be deemed too difficult of a task to automate correctly.
On the other hand, it could be deemed worth the risk if it protects others from physical harm. Answering such a question requires a deep understanding of the context in which the question is posed, the factors to be considered, and a systematic study of the implications of each potential answer under multiple and often uncertain scenarios. 

Given the relevance of ethics to robotics and closely related fields, in this lab we will explore a series of technical scenarios with inherent ethical problems, take a position, and develop arguments to support those positions.


# Learning Objectives

At the end of this lab, you should understand:
* How to synthesize meaningful questions from situations that present ethical problems.
* How to find and use related work that informs these questions.
* How to connect the current state of the art to possible future scenarios and their implications. 
* How to participate in debating ethical questions in a respectful productive way.

{% include notification.html message="Submission: The checkpoints are handled a little differently for this lab. The first checkpoint consist of a  written document that must be submitted via Collab before class on March 18th. The second checkpoint is the actual in-class debate on March 18th." 
status="is-success" 
icon="fas fa-exclamation-triangle" %}

#  Activity

The lab is divided in two parts: **Preparation** and **Debate**.

**Preparation.**
Start by forming a team with another class member (teams of 2) and pick one of the **prompts** provided in the next section. Then, your team must pick a side -- affirmative or negative -- and begin to build an argument around that position (more details on building the argument are found later in this doc).

Signup for teams, prompts, and affirmative/negative positions are available at [this google spreadsheet](https://docs.google.com/spreadsheets/d/14P6ccK_N2kmGguAlfEjwIMGkgzYyUN1Uk8nzB8UJiyc/edit?usp=sharing). Signup is on a first come, first serve basis so sign up quickly if you see a prompt you really like.
Note that teams on opposite positions of the same prompt will be debating against each other.

For example, let's say my team chooses a prompt like "Should businesses be allowed to make companionship robots for widespread commercial production?", and the position that I've chosen is affirmative; i.e. I've answered the question with a "Yes, they should be allowed to."
We would then look for sources to support  the positive  effects of having a companionship robot on the individual who buys one. For example, I might look at [social robots for the elderly](https://www.vox.com/future-perfect/2020/9/9/21418390/robots-pandemic-loneliness-isolation-elderly-seniors) and [interactive sex robots](https://www.forbes.com/sites/bernardmarr/2020/11/30/future-of-intimacy-sex-bots-virtual-reality-and-smart-sex-toys/?sh=39b5af7738fa) for those who struggle with intimacy, as well as what [healthcare professionals see as the benefits and drawbacks](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6719485/) of using them.
Then, I would point to possible future contexts in which the benefits and drawbacks of today might be amplified or fundamentally shifted, like the isolated, space-dwelling humans' relationships with their androids in Philip K. Dick's *[Do Androids Dream of Electric Sheep?](https://www.nature.com/articles/d41586-018-02695-7)*. 

**Debate.**
On the day of the lab, teams on opposite sides of the same prompt will debate each other and a winner will be chosen by the audience.
*Winning your debate won't affect your ability to earn a 100% on this lab (the lab is worth 5 points), but it will earn you one extra credit point towards your lab grade.*
Each debate will have 11 minutes with the time split evenly between teams and  a moderator to keep track of time (more details are provided below).




## Prompts

The prompts to choose from are:
1. Should [moral machines](https://www.media.mit.edu/publications/moral-machine-perception-of-moral-judgment-made-by-machines/) drive the analysis of risk when human life is involved? 
2. Should drivers have to take [alertness tests in order to use autonomous cars](https://www.nytimes.com/2019/03/28/business/autonomous-cars-technology-privacy.html), such as "alertness breathalyzers" to start the car or continuous monitoring of their eye movements?
3. Should a [caretaking/personal assistant robot](https://ieeexplore.ieee.org/abstract/document/5980058?casa_token=V_F525cBorwAAAAA:3_PgIxa4xJ2ne9vSWF_hKAGK6s7JrADljtNiuG42-5ZjcNPGjiZYGvAmDMk3YlOxgMha85uv) have control over the daily activities of their human (food, medicine, exercise, bedtime/waking time, etc.)? 
4. Should it be a war crime to [remotely operate robots that are equipped with the capability to use deadly force](https://dc.law.utah.edu/cgi/viewcontent.cgi?article=1047&context=ulr)?
5. Should the [right to repair](https://www.nytimes.com/2020/10/23/climate/right-to-repair.html) be available to everyone, if it means compromising potentially safety-critical systems?


## Gathering Sources to Build your Argument
 
We have provided a single source for you to help pick your topic and get started with your research, but you will need to incorporate more sources to understand your topic and formulate your argument. You can use as many additional sources as you need to support your position. To show that you have a well-informed position on the topic, you will need to do some background research and critical thinking about the sources you find. 

In the outline required for *Checkpoint 1* you must cite and include a variety of  sources that contribute to your argument. Here are the source types required with examples for the illustration prompt we mentioned earlier "Should businesses be allowed to make companionship robots for widespread commercial production?”:

1. At least two news articles that characterize the ways in which the target problem is present today.  Examples of this   might be the Vox article on [social robots for the elderly](https://www.vox.com/future-perfect/2020/9/9/21418390/robots-pandemic-loneliness-isolation-elderly-seniors) and the Forbes article on [interactive sex robots](https://www.forbes.com/sites/bernardmarr/2020/11/30/future-of-intimacy-sex-bots-virtual-reality-and-smart-sex-toys/?sh=39b5af7738fa) mentioned earlier.

2. At least one peer-reviewed writing such as a research article, journal, or book on ethics/fairness/discrimination in a field related to your prompt to back up your debate position. An example of this might be the [NIH study on what healthcare professionals see as the benefits and drawbacks of companion robots](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6719485/) mentioned earlier.

3. One motivating example that is either from an anecdotal account, or that has enough documented support to be a believably common occurrence. An example of this could be this human interest story from MIT on [sending robots to libraries to read to underprivileged children](https://www.media.mit.edu/projects/collaborative-robot-storyteller/overview/).

4. One **well-known** sci-fi/futuristic work of fiction that incorporates robots to use as a framing device for the future consequences or implications of the position you or your opponent may take on the prompt.   Favor  ones with which your audience may be familiar. For example, we might assume that most people have seen *2001: A Space Odyssey* and are passingly familiar with the concept of a robot controlling a space ship, but most people are not familiar with the Kubrickian interpretation of the ending in which the main character is supposedly held captive in an alien zoo. 
In the context of companion robots, a good choice of sci-fi could be Philip K. Dick's *[Do Androids Dream of Electric Sheep?](https://www.nature.com/articles/d41586-018-02695-7)* or "Dolores", the robot host from the 2016 reprise of TV series *[Westworld](https://www.imdb.com/title/tt0475784/)*. 
    
Think about your audience and plan carefully as you choose your source material, building them to address the questions of Checkpoint 1 (see below).  

You must cite (but not plagiarize) the research and arguments of others when building your related work.  


## Outlining Your Argument

When organizing your argument, you will have stronger and weaker points. Make sure your argument has a logical flow. 
Arguments are often organized according to points that the audience can easily agree with.
Getting audience "buy-in" early on can help when leading them along to your more complicated or difficult-to-accept arguments.

For example, there are lots of ways to structure an argument for an affirmative position to my example question "Should people be using companion robots en masse?". 
What's important is that I have solid reasons for why certain arguments are more compelling.
Let's say I have three arguments prepared:

<ol>
<li>Companion robots improve the quality of life for lots of people, the ones that they affect directly and many others that they affect indirectly. 
    <ol type="I">
    <li>Companion robots free up counselors and therapists to work on more complex problems.</li>
    <li>Companion robots help those with social and sexual disorders gain confidence, resulting in stronger human-to-human relationships. </li>
    </ol>
</li>
<li>Companion robots are excellent way to introduce robots into society and improve public opinion about robots.</li>
<li>Companion robots strengthen the economy and create jobs.</li>
</ol>

I've organized them this way because I believe I can confidently argue that the well-being of humans is the highest priority and the main purpose of companion robots, as well as an easy priority for much of my audience. That main point is followed by weaker points that I believe less people will think are important -- social opinion of robots and the health of the economy. 

Additionally, keep in mind the potential counterarguments. Acknowledging your opponent's points shows that you have a well thought out, 
comprehensive position and disarms your opponent ahead of time.
The negative position towards my example question might argue that companion robots might, through poor programming, hurt the people they are meant to be helping. 
Anticipating that, I might include a point in my argument that a companion robot only has the capabilities we equip it with, and its size and shape could be constrained to be light enough to be lifted by a human or to be made of only soft, bouncy materials that cannot damage people or structures.


# Checkpoint 1

Write up a 2-page outline of your position and how you intend to argue it, with all relevant sources cited. 
Your outline should consist of bulletpoints with a level of detail similar to the ones shown above  so you can easily reference this outline  during your debate.
In this outline, organize your arguments to address the following questions:
1. How is this problem relevant today? 
2. Why should the average person care? The average computer scientist?
3. What are the potential consequences/future outcome if your position is not followed? 
4. Does this position set a problematic precedent?
5. Can your position be legislated? Can that legislation be enforced? Can it be incentivized?
6. Under what circumstances might the counterarguments to your position be acceptable? Are those circumstances realistic? 

**This checkpoint needs to be handed in on Collab the day of the in-lab debate (March 18, 2021).** Part of the grade for this lab is based on how well you've prepared, and this outline is how we'll determine that.  


# Debate

Each debate will be 11 minutes long, broken down into an introduction to the topic, timed rebuttals, and a period of question-and-answer from the audience. 
At the end of the debate, the audience will be polled for who they think won, and the winner will earn extra credit points towards their lab grade.
The class has 10 teams, so there will be 5 debates with two teams per debate -- one affirmative, one negative.

The structure of each debate is as follows:
1. Introduction to the topic & opening points by speaker(s) with affirmative position: 2 minutes
2. Rebuttal & new points by speaker(s) with negative position: 2 minutes
3. Rebuttal & new points by speaker(s) with affirmative position: 2 minutes
4. Rebuttal, new points, and closing remarks by speaker(s) with negative position: 2 minutes
5. Question-and-answer from audience: 2 minutes
6. Poll audience via Zoom poll while the next debate speakers get set up: 1 minute

During the debate, keep these rules in mind:
* Moderators will cut you off at 2 minutes. Moderators will post "30 seconds", "10 seconds", and "time" in the Zoom chat to indicate the cutoff. Be mindful of this so you aren't cut off midsentence. 
* There is no time in between introduction and rebuttals, being prepared and thinking on your feet will be key to winning the argument
* Questions can be asked to either team, or both teams.


## Preparing for the Debate

When preparing for the debate  keep in mind the debate structure, delivery of your message, and that your responses stay on topic and within time. Do not read from notecards or prewritten pages, make eye contact with your audience, and keep in mind the length of your responses so the moderator does not cut you off mid-sentence.
We understand that there are certain cases where camera on might no be an option. We highly recommend that your camera is on to help engage with your audiance. Let the instructor know ahead of time if there is a reason your camera needs to be off.

You may want to do a dry run of your debate arguments  with your teammate or practice explaining the context of the question and your arguments to friends to see how a person unfamiliar with the topic would understand your presentation.

## Audience Questions

On the day of the debate, the audience will be asked to contribute questions to a Slack channel during each debate. 
Before the question-and-answer period, try to vote on your favorite question for each debate topic. 
Students with the highest-scoring questions will get time to ask their question during the question-and-answer period. 
Contributing questions on the Slack will contribute towards your participation grade.
We expect you to contribute at least one question per debate you are not apart of, and to upvote at least two other questions per debate.


#  Checkpoint 2
 
1. The speakers’ statements clearly supported their position. (20%)
2. The speakers’ statements appeared to be well researched and documented. (20%)
3. Arguments were presented with clarity and appropriate volume.  (20%)
4. Rebuttals were specific to opposing arguments and expressed with clarity.  (20%)
5. The speakers participated in asking questions for others’ debates (20%)
6. Outcome as per audience (+1 extra credit point)



# To Check

At the end of this lab, you should have the following:

1. An outline of your position and corresponding arguments.
2. A handful of resources for addressing questions of ethics in the field of robotics.
3. An understanding that these questions are ongoing and evolve with the technologies that provoke them.


 
# Extra
 
Here are just a few papers (list is a work in progress) from researchers who are currently working in topics of fairness, ethics, and discrimination in robotics and beyond, that may be good for background reading:
+ [Keith Abney](https://apps.dtic.mil/sti/pdfs/ADA534697.pdf)
+ [Fritz Allhoff](https://www.sciencedirect.com/science/article/pii/S2542660518300532)
+ [Ron Arkin](https://www.cc.gatech.edu/aimosaic/faculty/arkin/)
+ [George Bekey](https://apps.dtic.mil/sti/citations/ADA534697)
+ [Jean-François Bonnefon](https://science.sciencemag.org/content/352/6293/1573)
+ [Joanna J. Bryson](http://www.cs.bath.ac.uk/~jjb/ftp/Bryson19AIforLawofAI.pdf)
+ [Virginia Eubanks](https://openjournals.uwaterloo.ca/index.php/JoCI/article/download/2373/2916)
+ [Katya Klinova](https://arxiv.org/abs/2011.02787)
+ [Patrick Lin](https://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1004&context=phil_fac)
+ [Margaret Mitchell](https://arxiv.org/pdf/1801.07593)
+ [Iyad Rahwan](https://www.nature.com/articles/s41586-018-0637-6)
+ [Stuart Rusel](http://people.eecs.berkeley.edu/~russell/) 
+ [Kush Varshney](http://www.cs.cmu.edu/~rnoothig/papers/policy_orchestration.pdf)
+ [Aimee van Wynsberghe](https://link.springer.com/article/10.1007/s11948-011-9343-6)
+ [IEEE Ethics in Action](https://ethicsinaction.ieee.org/)
